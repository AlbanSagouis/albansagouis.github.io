<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-27T10:06:34+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">A personal website, a living CV</title><subtitle>This is a professional personal page, I speak of R programming,  scientific development, web development, reproducibility  and photography</subtitle><author><name>Alban Sagouis</name></author><entry><title type="html">Tech news 11: Facilitating‚Ä¶ admin tasks</title><link href="http://localhost:4000/blog/tech-news-11/" rel="alternate" type="text/html" title="Tech news 11: Facilitating‚Ä¶ admin tasks" /><published>2024-05-24T17:57:30+02:00</published><updated>2024-05-24T17:57:30+02:00</updated><id>http://localhost:4000/blog/tech-news-11</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-11/">&gt; Filling in PDF forms and sending personalised emails

### What for?

Maybe you want to pre-fill travel requests for all members of your group? Maybe you want to send a personalised email to all of your 200 co-authors, data providers, citizen science participants or parliament representatives? Maybe you want to send exam subjects to students but the data have to be different and randomly affected?

### A case study

While our manuscript was under review at GEB we decided to offer co-authorship to data providers that had not been contacted before because their data were open access. Why should they not be proposed to be coauthors while we propose it to people whom we had to contact to get access to their data, right?

GEB then asked us to send the the new list of co-authors to all co-authors, original and new. They would have to physically sign it, digital signatures are not acceptable. And this is a Wiley process, not just GEB, nut fun Wiley!

So anyway, two tasks that would be painful to do by hand: 1) reporting name, email address and institution of all 121 co-authors in a table with only nine rows and also reporting the names of the 49 new authors in another table with only five rows ; and 2) sending personalised messages to all of them.

### The data

Both processes could only be automated with good data and we had a table with separated first and family names, institutions and verified email addresses. It really helped that first names and family names were already in two columns and that multiple institutions were also in distinct columns.

### PDF form filling in R

At least the Wiley form had fields and after looking into the packages I already had on hand: `pdftools` and `qpdf`, I turned to DuckDuckGo that pointed me in direction of the `staplr` package. The installation was not painless, mostly because `staplr` depends on java and maybe there was a conflict with `pdftools` but once set, `staplr` did everything!

It has basic pdf tools such as `select_pages()`, `remove_pages()`, `rotate_pages()` and c`ombine_pdf()` that we used to extract the pages with the tables before multiplying the first table 14 times and the second one 10 times, filling them and finally combining all pages together.
For filling in the pdf, `staplr` works in three steps:

``` r
# First reading all fields in the document, there are 116 fields
&gt; fields &lt;- staplr::get_fields(input_filepath = &quot;Authorship-change-form--1---5---2--1.pdf&quot;)
&gt; fields[[50]] # this is Table 2, column `Email address`, row 6
$type
[1] &quot;Text&quot;

$name
[1] &quot;Email addressRow6&quot;

$value
[1] &quot;&quot;
# Then, filling in the fields in the value slot like this:
fields[[50]]$value &lt;- &quot;alban.sagouis@idiv.de&quot;
# Easy! But looping over 5 columns, 121 authors, changing page every 9 authors took some more tweaking‚Ä¶
# And finally writing the fields back inside the PDF document!
staplr::set_fields(
         input_filepath = &quot;Authorship-change-form--1---5---2--1.pdf&quot;,
         fields = fields,
         output_filepath = &quot;filled_authorship_form.pdf&quot;)
```

### Batch sending personalised emails

Now that we have the form ready, we can write to all co-authors, asking them to kindly print, sign, scan and send back the form.
Our message would begin with ‚ÄúDear Dr {Family name author}, thank you for contributing data from your data set entitled {Dataset title}, et caetera‚Äù. I quickly looked into how to do it in R and the prospect of setting up an email server seemed too complicated, especially when Microsoft Word and Outlook can do it.

Just write your message in a Word document, click on the Mailings tab, Start Mail Merge, choose Letters or Email messages and then click on Select Recipients and select the table where you stored the names, the titles, the personalised messages, random data for an exam, everything you need personalised and of course, the email addresses. Every time you reach a part of the email where you want personalised text, just Insert Merge Field and select the column from your table where to look for data. Once you‚Äôre done composing the email, you can Preview Results and go through all 121 recipients to check that the good name or title is sent to the good email address or check that special characters were properly reproduced.

It‚Äôs that simple! Finish &amp; Merge, Word and Outlook are going to talk to each other and send each email independently.

Oh, you want to send an attachment too? Or even send a different attachment to each person? Well, Word and Outlook won‚Äôt give you the option ü§∑üèª‚Äç‚ôÇÔ∏è. We tried a first add-in for Word whose free plan did not fit our needs (max 50 recipients) and then found the Merge Tools add-in that allows batch sending emails 20 times for free, thanks Merge Tools.

### Encoding

When reading data, Word preferred `csv` to `xlsx` but encoding handling was better from an Excel file‚Ä¶
For one of the steps, Merge Tools seemed to mishandle UTF-8 encoding and I had to remove all special characters ie. &quot;√®√©√™√´ƒõ&quot; all became &quot;e&quot;. I used this R command:

``` r
stringi::stri_trans_general(
   str = your_string_with_special_characters, 
   id = ‚ÄúLatin-ASCII&quot;)
```

If you have `tidyverse` installed, you already have `stringi`.

### Resources

The staplr package: &lt;https://cran.r-project.org/web/packages/staplr/&gt;

The microsoft Support page for Mail Merge: &lt;https://support.microsoft.com/en-us/office/use-mail-merge-to-send-bulk-email-messages-0f123521-20ce-4aa8-8b62-ac211dedefa4&gt;

A quick video to see it happening: &lt;https://www.youtube.com/watch?v=NikC2cJ_tHQ&gt;

The Merge Tools add-in that we ended up using: &lt;https://mergetoolsaddin.com/&gt;</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><summary type="html">Filling in PDF forms and sending personalised emails</summary></entry><entry><title type="html">Tech news 10: Pipes</title><link href="http://localhost:4000/blog/tech-news-10/" rel="alternate" type="text/html" title="Tech news 10: Pipes" /><published>2024-04-26T21:24:30+02:00</published><updated>2024-04-26T21:24:30+02:00</updated><id>http://localhost:4000/blog/tech-news-10</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-10/">### What for?

Pipes are present is many languages and they allow passing objects from one function to another without having to create an intermediary object and keeping a logical and readable flow.
In the command line, you can read all files names in a folder, pass them to grep to select all ‚Äú.R‚Äù files and pass them all to Rscript to execute them. Instead of creating intermediate files, the result of each function is passed to the next by a pipe, represented by ‚Äú|‚Äù in bash.

In R, there were two classical ways of going through a workflow.
Creating intermediate objects even though not useful to keep:

``` r
# Abundances in three sites
dt &lt;- data.frame(abundance = 20:50,
                 site = rep(
                    c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;),
                    each = 10))

sitesAB &lt;- subset(dt, site %in% c(&quot;A&quot;, &quot;B&quot;))
subSample &lt;- sample(sitesAB$abundance, size = 5)
sortedAbundance &lt;- sort(subSample,
                        descending = FALSE)
```

Or nesting functions with the disadvantage that you have to read the workflow from the inside to the outside and arguments of the same function are sometimes far from each other, a hard-to-digest-sandwich‚Ä¶

``` r
sortedAbundance &lt;- sort(
                     sample(
                       subset(dt,
                              site %in% c(&quot;A&quot;, &quot;B&quot;))),
                       size = 5),
                     descending = FALSE)
```

Using a pipe, it looks like this:

``` r
sortedAbundance &lt;- dt %&gt;%
  subset(site %in% c(&quot;A&quot;, &quot;B&quot;)) %&gt;%
  sample(size = 5) %&gt;%
  sort(descending = FALSE)
```

Easier to read, intention is clear, arguments stay closer to each other and input and output objects are also close to each other: `dt` is transformed into `sortedAbundance`.

The pipe takes the object passed to it and passes it as the first argument of the next function. If we need to pass the piped object to the second argument, we can name arguments or use a place holder:
``` r
c(&quot;A&quot;,&quot;B&quot;) %&gt;% sub(pattern = ‚ÄúA‚Äù,
                   replacement = ‚Äú‚Äù)
c(&quot;A&quot;,&quot;B&quot;) %&gt;% sub(pattern = ‚ÄúA‚Äù,
                   replacement = ‚Äú‚Äù,
                   x = .)
```

### The `magrittr` pipe
The `magrittr` package with its iconic `%&gt;%` pipe, was first published early 2014, apparently it caught traction pretty quick and Rstudio developers contacted the creator: ‚ÄúWe also worked on a pipe `%.%` but it‚Äôs not as functional, practical and rich as yours, could we collaborate?‚Äù

This pipe, together with the tidyverse grammar then revolutionised the R ecosystem‚Ä¶

### The base pipe
And eventually, R developed its own native pipe `|&gt;`. In appearance, its usage is very similar with one difference being that the placeholder is ‚Äú_‚Äù instead of ‚Äú.‚Äù.
You can read more in this [Hadley Wickam article][article] or in the pipe section of his [book][book] in which he recommends `base` pipe over `maggritr` pipe:
&gt; For simple cases, `|&gt;` and `%&gt;%` behave identically. So why do we recommend the base pipe? Firstly, because it‚Äôs part of base R, it‚Äôs always available for you to use, even when you‚Äôre not using the tidyverse. Secondly, `|&gt;` is quite a bit simpler than `%&gt;%`: in the time between the invention of `%&gt;%` in 2014 and the inclusion of `|&gt;` in R 4.1.0 in 2021, we gained a better understanding of the pipe. This allowed the base implementation to jettison infrequently used and less important features.

Partly because the `base` pipe is simpler, it has no overhead and is much faster than the `maggritr` pipe:

``` r
R&gt; system.time({for(i in 1:1e5) identity(x)})
   user  system elapsed 
  0.015   0.000   0.015 
R&gt; system.time({for(i in 1:1e5) x |&gt; identity()})
   user  system elapsed 
  0.015   0.000   0.015 
R&gt; system.time({for(i in 1:1e5) x %&gt;% identity()})
   user  system elapsed 
  0.105   0.001   0.106
```

### The other pipes
Less frequent as the well-known forward pipe `%&gt;%`, the `maggritr` package offers other pipes!

 - The assignment pipe `%&lt;&gt;%`: Pipe an object forward into a function or call expression and update the left-hand-side object with the resulting value.

``` r
dt %&lt;&gt;% mean() # equivalent to dt &lt;- dt %&gt;% mean()
```

 - The exposition pipe `%$%`: Expose the names in left-hand-side to the right-hand-side expression. This is useful when functions do not have a built-in data argument.

``` r
iris %&gt;%
  subset(Sepal.Length &gt; mean(Sepal.Length)) %$%
  cor(Sepal.Length, Sepal.Width)
#&gt; [1] 0.3361992
```

 - The tee pipe `%T%`: Pipe a value forward into a function- or call expression and return the original value instead of the result. This is useful when an expression is used for its side-effect, say plotting or printing.

``` r
rnorm(200) %&gt;%
  matrix(ncol = 2) %T&gt;%
  plot() %&gt;% 
  colSums()
```

### Resources

The pipe article by Hadley Wickam: &lt;https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/&gt;

The pipe article in the R for Data Science book (Hadley Wickam, second edition): &lt;https://r4ds.hadley.nz/data-transform.html#sec-the-pipe&gt;

[article]:  https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/
[book]:     https://r4ds.hadley.nz/data-transform.html#sec-the-pipe</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><summary type="html">What for?</summary></entry><entry><title type="html">Tech news 9: Testing data</title><link href="http://localhost:4000/blog/tech-news-9/" rel="alternate" type="text/html" title="Tech news 9: Testing data" /><published>2024-03-15T01:37:30+01:00</published><updated>2024-03-15T01:37:30+01:00</updated><id>http://localhost:4000/blog/tech-news-9</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-9/">### Why testing data?

Looking inside the data you receive and produce is an absolute necessity but if you could have a second pair of eyes able to scan millions of rows in fractions of seconds and as often as needed, why not? Plus buildings your tests, ie data checks needs you to think forward to define your expectations for data and this helps controlling the workflow and ensuring data quality at each step.

### How?

In software development, unit tests check that a function always behaves as expected. Unit tests can be extended to check data too, ie:

 - Are column names equal to ‚Äúyear‚Äù and ‚Äúsite‚Äù AND are all ‚Äúyear‚Äù values positive integers &gt; 1998 AND is the ‚Äúsite‚Äù column of type factor?
 - Are the X trait values for species 1 in the range 10:20 AND in the range 12:30 for species 2?

By scripting as many tests as needed, we effortlessly make sure that entry data quality is optimal and that it stays correct along data processing. It is more reproducible and time saving.

### `Assertr` for inline/in-workflow testing

An example of input data quality control from the `assertr` documentation:

&gt; Let‚Äôs say, for example, that the R‚Äôs built-in car dataset, `mtcars`, was not built-in but rather procured from an external source that was known for making errors in data entry or coding. Pretend we wanted to find the average miles per gallon for each number of engine cylinders. We might want to first, confirm
&gt; - that it has the columns &quot;mpg&quot;, &quot;vs&quot;, and &quot;am&quot;
&gt; - that the dataset contains more than 10 observations
&gt; - that the column for &apos;miles per gallon&apos; (mpg) is a positive number
&gt; - that the column for ‚Äòmiles per gallon‚Äô (mpg) does not contain a datum that is outside 4 standard deviations from its mean, and
&gt; - that the ‚Äúam‚Äù and ‚Äúvs‚Äù columns (automatic/manual and v/straight engine, respectively) contain 0s and 1s only
&gt; - each row contains at most 2 NAs
&gt; - each row is unique jointly between the &quot;mpg&quot;, &quot;am&quot;, and &quot;wt&quot; columns
&gt; - each row&apos;s mahalanobis distance is within 10 median absolute deviations of all the distances (for outlier detection)
&gt; This could be written (in order) using `assertr` like this:

``` r
library(dplyr)
library(assertr)

mtcars %&gt;%
  verify(has_all_names(&quot;mpg&quot;, &quot;vs&quot;, &quot;am&quot;, &quot;wt&quot;)) %&gt;%
  verify(nrow(.) &gt; 10) %&gt;%
  verify(mpg &gt; 0) %&gt;%
  insist(within_n_sds(4), mpg) %&gt;%
  assert(in_set(0,1), am, vs) %&gt;%
  assert_rows(num_row_NAs, within_bounds(0,2), everything()) %&gt;%
  assert_rows(col_concat, is_uniq, mpg, am, wt) %&gt;%
  insist_rows(maha_dist, within_n_mads(10), everything()) %&gt;%
  group_by(cyl) %&gt;%
  summarise(avg.mpg=mean(mpg))
```
&gt; If any of these assertions were violated, an error would have been raised and the pipeline would have been terminated before calculation happened.

In this workflow, `assertr` was used for inline testing, it is immediately apparent, transparent and clear. But maybe you would rather have a suite of tests in a separate script that you would `source()` or that you would call with the dedicated and enriched with clear and helpful error messages `testthat::test_file()`.

### Test suites called by `testthat`

`testthat` was developed for software unit testing and it is the reference in R package building but extensions make it a highly efficient data testing tool. The usual `testthat` test suite is structured around expectations:
 - the `rpois` function is expected to error if argument x has NA
 - the `rpois` function is expected to return a vector of positive integers without NAs
``` r
library(testthat)
test_that(‚Äúrpois behaves as expected‚Äù, {
   expect_error(rpois(n = NA, 10))

   expect_false(anyNA(rpois(n = 5,10)))
   expect_vector(rpois(n = 5,10), size = 5)
   expect_gte(rpois(n = 5, 10), 0))
   expect_type(rpois(n = 5, 10), ‚Äúinteger‚Äù)
})
```

Now, if we could have more data oriented tests, easily applied to data frames, it would feel more natural than multiplying `testthat` expectations. For example, the last four `testthat` expectations could be rewritten with only one `checkmate` expectation:

``` r
checkmate::expect_integer(rpois(n = 5, 10), lower = 0, len = 5, any_missing = FALSE)
```

Another useful `checkmate` function to check column names:
Expecting all column names in any order

``` r
expect_names(
  permutation.of = c(‚Äúregion‚Äù,‚Äùsite‚Äù,‚Äùplot‚Äù,‚Äùyear‚Äù,‚Äùmonth‚Äù),
  what = &quot;colnames&quot;
)
```

Or expecting all names AND in order

``` r
expect_names(
  identical.to = c(‚Äúregion‚Äù,‚Äùsite‚Äù,‚Äùplot‚Äù,‚Äùyear‚Äù,‚Äùmonth‚Äù),
  what = &quot;colnames&quot;
)
```

Or allowing only a subset of a list

``` r
expect_names(
  subset.of = c(‚Äúregion‚Äù,‚Äùsite‚Äù,‚Äùplot‚Äù,‚Äùyear‚Äù,‚Äùmonth‚Äù),
  what = &quot;colnames&quot;
)
```

`Checkmate` was developed with focus on helpful error messages and efficiency:

&gt; Virtually every standard type of user error when passing arguments into function can be caught with a simple, readable line which produces an informative error message in case. A substantial part of the package was written in C to minimize any worries about execution time overhead. Furthermore, the package provides over 30 expectations to extend the popular `testthat` package for unit tests.

Now, let‚Äôs use `testdat`, another extension to `testthat`, to build a testing suite for data. First, the `testdat` authors give an example workflow in which the user creates data objects and visually and interactively checks them:

``` r
library(dplyr)

x &lt;- tribble(
  ~id, ~pcode, ~state, ~nsw_only,
  1,   2000,   &quot;NSW&quot;,  1,
  2,   3123,   &quot;VIC&quot;,  NA,
  3,   2123,   &quot;NSW&quot;,  3,
  4,   12345,  &quot;VIC&quot;,  3
)

# check id is unique
x %&gt;% filter(duplicated(id))

# check values
x %&gt;% filter(!pcode %in% 2000:3999)
x %&gt;% count(state)
x %&gt;% count(nsw_only)

# check base for nsw_only variable
x %&gt;% filter(state != &quot;NSW&quot;) %&gt;% count(nsw_only)

x &lt;- x %&gt;% mutate(market = case_when(pcode %in% 2000:2999 ~ 1,
                                     pcode %in% 3000:3999 ~ 2))

x %&gt;% count(market)
```
And then using `testdat`:
``` r
library(testdat)
library(dplyr)

x &lt;- tribble(
  ~id, ~pcode, ~state, ~nsw_only,
  1,   2000,   &quot;NSW&quot;,  1,
  2,   3123,   &quot;VIC&quot;,  NA,
  3,   2123,   &quot;NSW&quot;,  3,
  4,   12345,  &quot;VIC&quot;,  3
)

with_testdata(x, {
  test_that(&quot;id is unique&quot;, {
    expect_unique(id)
  })
  
  test_that(&quot;variable values are correct&quot;, {
    expect_values(pcode, 2000:2999, 3000:3999)
    expect_values(state, c(&quot;NSW&quot;, &quot;VIC&quot;))
    expect_values(nsw_only, 1:3) # by default expect_values allows NAs
  })
  
  test_that(&quot;filters applied correctly&quot;, {
    expect_base(nsw_only, state == &quot;NSW&quot;)
  })
})
```

Where attention is needed only if a test fails‚Ä¶ these tests can be stored in a separate script and called with the data they are applied to.
Finally, in heavy data workflows where input data, received from data providers or automatic loggers, follows a limited number of formats, full data check suites can be automatically ran on reception with rich and parametrisable reporting and this is what pointblank was developed for.

Checking the data your data providers send/your automatic inputs

#### You can also use `pointblank` in your script as shown here:
``` r
library(pointblank)
dplyr::tibble(
    a = c(5, 7, 6, 5, NA, 7),
    b = c(6, 1, 0, 6,  0, 7)
  ) %&gt;%
  col_vals_between(
    a, 1, 9,
    na_pass = TRUE,
    actions = warn_on_fail()
  ) %&gt;%
  col_vals_lt(
    c, 12,
    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b),
    actions = warn_on_fail()
  ) %&gt;%
  col_is_numeric(
    c(a, b),
    actions = warn_on_fail()
  )
```

But a better use for it is creating so-called agents that can automatically and reproducibly test data and provide rich visual reports just like this:
``` r
agent &lt;- 
  dplyr::tibble(
    a = c(5, 7, 6, 5, NA, 7),
    b = c(6, 1, 0, 6,  0, 7)
  ) %&gt;%
  create_agent(
    label = &quot;A very *simple* example.&quot;,
    actions = al
  ) %&gt;%
  col_vals_between(
    vars(a), 1, 9,
    na_pass = TRUE
  ) %&gt;%
  col_vals_lt(
    vars(c), 12,
    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b)
  ) %&gt;%
  col_is_numeric(vars(a, b)) %&gt;%
  interrogate()
print(agent)
```

And the `pointblank` package currently supports PostgreSQL. MySQL, MariaDB, Microsoft SQL Server, Google BigQuery, DuckDB, SQLite, and Spark DataFrames!

### Resources
&lt;https://ropensci.r-universe.dev/assertr#&gt;
&lt;https://testthat.r-lib.org/&gt;
&lt;https://socialresearchcentre.r-universe.dev/testdat#&gt;
&lt;https://socialresearchcentre.github.io/testdat/articles/testdat.html&gt;
&lt;https://rstudio.r-universe.dev/pointblank#&gt;</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><summary type="html">Why testing data?</summary></entry><entry><title type="html">Saving the camera settings of a shot in the exif data of the scans</title><link href="http://localhost:4000/blog/saving-shot-metadata/" rel="alternate" type="text/html" title="Saving the camera settings of a shot in the exif data of the scans" /><published>2024-02-26T01:15:30+01:00</published><updated>2024-02-26T01:15:30+01:00</updated><id>http://localhost:4000/blog/saving-shot-metadata</id><content type="html" xml:base="http://localhost:4000/blog/saving-shot-metadata/">For decades (centuries?) photographers have been writing their camera settings for each shot on a
notebook to learn and train their eye, to organise their shots or because they are data-freaks.
Today, you could still make notes by hand or use a dedicated smartphone app or you could be the
proud owner of a more modern film camera that saves the shot data on a memory card.

Once the film developed, by yourself or a lab, and scanned, by yourself or a lab, the scans you get
do not have the information of the original shots. These data are on paper, on a smartphone or on a memory card.
I downloaded the `Analog` app for my smartphone, because it is free, does not collect user data,
it&apos;s lightweight and I loved the design, and I began saving the settings for each shot. Data was accumulating
but I was not seeing how to get it out yet.

A digital camera would take the picture and save the settings inside the `jpg` or `tiff` files
in exif data. I want the same thing for my scans!

And how to get these data in the exif fields of the scans? Well surprisingly, it&apos;s not as straightforward as
you (I) would expect. At first I did not find anything satisfying and discussing with the developers behind
`Analog` made it clear that it was not a straightforward task for them either. We talked about workflow.

The team were encouraging amateur and professional photographers to rename their
files, well their scans, following this convention to save the settings of
each shot. Then they developed `Analog` which of course allows users to save Shutter speed,
Aperture, the lens&apos; focal length and exposure correction. At the end of the
roll, the data is extracted and the user receives a list of name files by email:

&gt; Test roll_NO01_SS50_A2.8_FL50_EX0
&gt; Test roll_NO02_SS50_A2.8_FL50_EX0
&gt; Test roll_NO03_SS125_A4_FL50_EX0
&gt; Test roll_NO04_SS125_A4_FL50_EX0

They hand-rename the scans or use a renaming software. OK.
but this does not go _inside_ the files, a file name is not an ideal way
of storing data even when using the standardised convention they created:
NOSSAFLEX

&gt; It‚Äôs as easy as the name ‚Äì NOSSAFLEX has all of the information in the
&gt; title.
&gt;
&gt; NO = Number  
&gt; SS = Shutter Speed  
&gt; A = Aperture  
&gt; FL = Focal Length  
&gt; EX = Exposure

I could not find a way to automatically save the data on my phone to the exif slots of my scans...

So I decided to create an R package to do it! An exciting weekend project and certainly
an article for the blog!

Since I was already using the `Analog` app, I had to follow its data structure.

### Here is the README of my package

This package relies on a file naming convention for, essentially analog,
photography. Photographs taken on film, then developed, then scanned are lacking
important metadata or if they have, it&apos;s from the scanning device, not the
original camera.
The nossaflex package takes NOSSAFLEX structured file names
and can 1) batch rename corresponding pictures and 2) edit their exif data
to add information such as focal length, shutter speed, aperture.
This allow a photographer to take notes on the `Analog` app while shooting
pictures, export nossaflex compatible file names and use R to rename scans
and edit their exif metadata with corresponding shot information.

The nossaflex package is the entry of the NOSSAFLEX file naming
convention into the R universe to help scientific, professional or
amateur photographers to name their picture files consistently and
informatively.

When taking a picture with an analog camera, data such as aperture,
shutter speed or focal length are not automatically saved the way they
are in a digital camera. Many photographers write down these precious
metadata in a notebook, we want to help them improve their workflow and
data quality.

#### What is NOSSAFLEX?

Here is an explanation from the creators:

&gt; It‚Äôs as easy as the name ‚Äì NOSSAFLEX has all of the information in the
&gt; title.
&gt;
&gt; NO = Number  
&gt; SS = Shutter Speed  
&gt; A = Aperture  
&gt; FL = Focal Length  
&gt; EX = Exposure

NOSSAFLEX file names looks like this: `NO03_SS250_A8_FL80_EX0.jpg` or
this: `NO34_SS30_A2.8_FL35_EX+1.tiff`!

Learn more on their \[website\]{&lt;https://nossaflex.io/the-system&gt;} or on
their \[Youtube channel\]{&lt;https://www.youtube.com/@NOSSAFLEX&gt;}.

#### The package

Here are the two main functions in the package:

- `renaming_nossaflex` batch-renames picture files from uninformative
  `DSC_00345.jpg` to information-rich NOSSAFLEX name based on data
  provided by the user, see {analog} section:
  `NO03_SS250_A8_FL80_EX0.jpg`.
- `editing_exif` batch-saves the metadata of the pictures into the exif
  slots of the scan files (jpg, tiff, etc).

#### Analog or an other app

#### The workflow

#### Installation

You can install the development version of nossaflex from
[GitHub](https://github.com/) with:

``` r
# install.packages(&quot;devtools&quot;)
# devtools::install_github(&quot;AlbanSagouis/nossaflex&quot;)
```

#### Example

This is a basic example which shows you how to solve a common problem:

``` r
library(nossaflex)
files &lt;- c(&quot;Pictures/2024/01 02 Winter in Berlin/DSC_001034&quot;,
           &quot;Pictures/2024/01 02 Winter in Berlin/DSC_001035&quot;,
           &quot;Pictures/2024/01 02 Winter in Berlin/DSC_001036&quot;)
filenames &lt;- reading_nossaflex(path = &quot;path_to_the_filenames.txt&quot;) # provided by the `analog` app
renaming_nossaflex(filenames = filenames, files = files)
```

Additionally you may want to safely save the shots metadata inside the
scans:

``` r
metadata &lt;- reading_nossaflex(path = &quot;path_to_the_filenames.txt&quot;) |&gt;  # provided by the `analog` app
     parsing_nossaflex()
editing_exif(files, metadata)
```

#### Related work

The package relies heavily on the great
`exiftoolr`{&lt;https://github.com/JoshOBrien/exiftoolr/&gt;} package by
@JoshOBrien which itself depends on the great
`exiftool`{&lt;https://exiftool.org/&gt;} software by Phil Harvey.  
Finally, jExifToolGUI{&lt;https://github.com/hvdwolf/jExifToolGUI&gt;} also
offers exif editing and with a Graphical Interface, nice.</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="photography" /><category term="weekend-project" /><category term="experience" /><category term="R" /><category term="package" /><summary type="html">For decades (centuries?) photographers have been writing their camera settings for each shot on a notebook to learn and train their eye, to organise their shots or because they are data-freaks. Today, you could still make notes by hand or use a dedicated smartphone app or you could be the proud owner of a more modern film camera that saves the shot data on a memory card.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/saving-shot-metadata.png" /><media:content medium="image" url="http://localhost:4000/assets/images/saving-shot-metadata.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Tech news 8: Sharing code through your own R package</title><link href="http://localhost:4000/blog/tech-news-8/" rel="alternate" type="text/html" title="Tech news 8: Sharing code through your own R package" /><published>2024-02-16T22:37:30+01:00</published><updated>2024-02-16T22:37:30+01:00</updated><id>http://localhost:4000/blog/tech-news-8</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-8/">This month, I thought of writing a little bit about sharing code with each other.

### Collaborating on code

Let‚Äôs imagine having a workshop with a group of colleagues and over the course of the week, some will develop tools and some will use said tools for various analyses. The workflow will most likely be:

1. let‚Äôs define our needs, some functions, the data they take in input and what we all need out
2. some begin writing functions, each their own maybe and then share with everyone,
3. the code is emailed around, saved on a USB key or shared on DropBox
4. a. users test the functions, find bugs, need improvements, need more details on input format, give feedback
4. b. In the meantime, programers fixed some things, fixed functions, created others and back to 2.

It‚Äôs hard keeping track of the versions and consistently and easily distributing updates to users. It‚Äôs hard distributing documentation, testing functions reproducibly and controlling the dependencies.
And if you‚Äôre teaching and want to distribute code and data all in one?

### Make your life easier, make a package

If the functions were written in a package, documentation and tests can easily be written and processed thanks to rstudio built-in tools. The users can install the package, from the shared dropbox or from github/gitlab almost as usual, and load it with `library(yourPackage)` as usual. They can access the help documentation you wrote with ‚Äú?‚Äù as they are used to, read examples, enjoy auto-completion of the arguments, etc.
When they give you feedback, the version is clear and the code changes can be tested in seconds with a shortcut.

You can also share data inside the package, and again, the version will be a timestamp and get you a reproducibility badge. It‚Äôs also going to be very easy to access for the users:

``` r
library(yourPackage)
dataName
```

### How to make a package?

There are so many great (better) resources online! But let me just give a couple of tips:  

The absolute minimum that makes an R package an R package is one DESCRIPTION file and one R script, that‚Äôs it!

Begin small, discover the thrill, clarity and comfort of writing documentation and tests for your functions. This will give you ways of imagining edge cases, eg thinking of the missing values, and controlling. Make a change, Ctrl+Shift+T(est), make another change, Ctrl+Shift+T, etc!

&gt; Make a change, Ctrl+Shift+T(est), make another change, Ctrl+Shift+T, etc!

Take advantage of tools you already have on your computer: RStudio, the devtools, `testthat` packages for example.

For whatever questions you might have while building, documenting or testing, the first DuckDuckGo result will very likely comes from Wickham‚Äôs book (&lt;https://r-pkgs.org/&gt;), it‚Äôs just so complete‚Ä¶

Wanna do it differently? Build the tests first to build up the framework of exactly how you want your function to behave, then write the function.

### Some useful tools

- `devtools::document()` will build the documentation, help pages, vignettes everything
- `devtools::install()`, `devtools::install(‚Äú/mydropbox/myPackage‚Äù)` or `devtools::install_github(‚ÄúyourRepo/yourPackage‚Äù)` will install the package from source from wherever is the code.
- `testthat::test_package()` will run all of your tests and give you encouraging words when they fail, how thoughtful!

I hope this decreased the barrier of trying out building a package. I hope however that it is not decreasing the impressiveness of it!

Best wishes,

Alban</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><summary type="html">This month, I thought of writing a little bit about sharing code with each other.</summary></entry><entry><title type="html">Tech news 7: Creating animated figures</title><link href="http://localhost:4000/blog/tech-news-7/" rel="alternate" type="text/html" title="Tech news 7: Creating animated figures" /><published>2024-01-26T16:57:30+01:00</published><updated>2024-01-26T16:57:30+01:00</updated><id>http://localhost:4000/blog/tech-news-7</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-7/">This week‚Äôs email is about `tidymodels`.
&gt; The `tidymodels` framework is a collection of packages for modeling and machine learning using `tidyverse` principles.

`Tidymodels` offers a consistent and flexible framework for your data science and data programming needs. This tool suite is designed to streamline and simplify the process of building and tuning models, making it easier for researchers to extract meaningful insights from their data.

To illustrate the use of key packages in `tidymodels`, here&apos;s a short example workflow:

  `Workflow (`The workflow package enables you to assemble the individual components of your modeling process into a cohesive workflow. You can define the order of operations and specify the preprocessing steps, model fitting, and model evaluation`) {`

  `Recipes (`The recipes package helps with data preprocessing and feature engineering. You can create a recipe to define the steps for encoding categorical variables, scaling numeric variables, and creating new features`) %&gt;%`

  `Dials (`The dials package provides a way to define tuning parameters for models. You can create a set of possible values for each parameter using the `grid_*()` functions`) %&gt;%`

  `Parsnip (`The parsnip package allows you to specify the type of model you want to build. For example, you can create a linear regression model using `linear_reg()` function`) %&gt;%`

  `Tune (`The tune package provides functions for hyperparameter tuning. You can use the &apos;tune_*()&apos; functions to automatically search for the best combination of hyperparameters for your model using a specified tuning grid`)`
`}`

By utilising these packages in sequence, you can build a comprehensive modeling pipeline that includes specifying the model type, preprocessing data, tuning hyperparameters, and evaluating model performance.

Here is a code example from the tidymodels ‚ÄúGet started‚Äù page:
``` r
urchins &lt;- # Loading the data
  readr::read_csv(&quot;https://tidymodels.org/start/models/urchins.csv&quot;) %&gt;% 
  stats::setNames(c(&quot;food_regime&quot;, &quot;initial_volume&quot;, &quot;width&quot;)) %&gt;% 
  dplyr::mutate(food_regime = factor(food_regime, levels = c(&quot;Initial&quot;, &quot;Low&quot;, &quot;High&quot;)))

# Then preparing the model: a linear regression
parsnip::linear_reg() %&gt;% # This is the model type
  parsnip::set_engine(&quot;keras&quot;)  # This is the engine / fitting method
lm_mod &lt;- parsnip::linear_reg() # The default engine: ‚Äúlm‚Äù is used

# And now we fit the model:
lm_fit &lt;- 
  lm_mod %&gt;% 
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
#&gt; parsnip model object
#&gt; 
#&gt; 
#&gt; Call:
#&gt; stats::lm(formula = width ~ initial_volume * food_regime, data = data)
#&gt; 
#&gt; Coefficients:
#&gt;                    (Intercept)                  initial_volume  
#&gt;                      0.0331216                       0.0015546  
#&gt;                 food_regimeLow                 food_regimeHigh  
#&gt;                      0.0197824                       0.0214111  
#&gt;  initial_volume:food_regimeLow  initial_volume:food_regimeHigh  
#&gt;                     -0.0012594                       0.0005254

# broom::tidy() offers a clean output:
broom::tidy(lm_fit)
#&gt; # A tibble: 6 √ó 5
#&gt;   term                                            estimate std.error statistic  p.value
#&gt;   &lt;chr&gt;                                          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
#&gt; 1 (Intercept)                                   0.0331     0.00962      3.44  0.00100 
#&gt; 2 initial_volume                                0.00155   0.000398      3.91  0.000222
#&gt; 3 food_regimeLow                                0.0198      0.0130      1.52  0.133   
#&gt; 4 food_regimeHigh                               0.0214      0.0145      1.47  0.145   
#&gt; 5 initial_volume:food_regimeLow                -0.00126   0.000510     -2.47  0.0162  
#&gt; 6 initial_volume:food_regimeHigh                0.000525  0.000702      0.748 0.457
```

Now what if we wanted something different like a Bayesian model? No need to change everything, tidymodels functions are generic even though the underlying packages use very different syntax:
``` r
# set the prior distribution
prior_dist &lt;- rstanarm::student_t(df = 1)

# make the parsnip model
bayes_mod &lt;-   
  parsnip::linear_reg() %&gt;% 
  parsnip::set_engine(&quot;stan&quot;, 
             prior_intercept = prior_dist, 
             prior = prior_dist) 

# train the model
bayes_fit &lt;- 
  bayes_mod %&gt;% 
  fit(width ~ initial_volume * food_regime, data = urchins)

Easily create training and testing data sets with rsamples:
urchins_split &lt;- rsamples::initial_split(urchins %&gt;% select(-width), 
                            strata = class)
urchins_train &lt;- rsamples::training(urchins_split)
urchins_test  &lt;- rsamples::testing(urchins_split)
Create a Workflow for even more reproducibility and reliability:
urchins_wflow &lt;- 
  workflow::workflow() %&gt;% 
  workflow::add_model(lm_mod) %&gt;% 
  workflow::add_recipe(urchins_rec)
Fit the model on the training data:
urchins_fit &lt;- 
  urchins_wflow %&gt;% 
  fit(data = train_data)
And easily fit and predict on the test data!
predict(urchins_fit, test_data)
```

I hope this triggered your curiosity and you see potential for an increase in efficiency and reliability in your analytical work!

I relied heavily on the `tidymodels` ‚ÄúGet started‚Äù page to prepare this email and I encourage anyone interested to consult it too or even follow the introductory or advanced workshops the authors published online!

### Resources

The tidyverse blog is a great resource for staying up to date with the latest tidymodels news and developments:  

- Tidymodels website: &lt;https://www.tidymodels.org/&gt;
- Tidymodels get started: &lt;https://www.tidymodels.org/start/&gt;
- Tidymodels workshops: &lt;https://workshops.tidymodels.org/&gt;

Happy to discuss it and collaborate!
Best wishes,
Alban</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><summary type="html">This week‚Äôs email is about tidymodels. The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.</summary></entry><entry><title type="html">My discovery of analog photography</title><link href="http://localhost:4000/blog/discovery-analog-photography/" rel="alternate" type="text/html" title="My discovery of analog photography" /><published>2024-01-11T01:15:30+01:00</published><updated>2024-01-11T01:15:30+01:00</updated><id>http://localhost:4000/blog/discovery-analog-photography</id><content type="html" xml:base="http://localhost:4000/blog/discovery-analog-photography/">&lt;p&gt;I began using digital cameras a long time ago without trying to understand the technicalities of it until I got serious about bird photography. Then I really got into the technicalities‚Ä¶ shooting thousands of photographs trying to improve my technique and to get beautiful and tricky shots of birds. I certainly had a lot of fun doing it but at one point I needed something different, something simpler and most importantly something more creative.&lt;/p&gt;

&lt;p&gt;I began looking for an analog camera and I received a Nikon EM with a Nikon Nikkor series E 50mm 1/1.8 as a present. An analog camera with actual film that you buy and you pay to develop, with silver ions and toxic chemistry that made me think each shot and make only one. A prime lens that would force me to move closer or further by walking. Finally I could take the time to think about the composition of the shot instead of taking as many pictures as fast as I could before a bird would take off. And black and white film with an explosion of contrast!&lt;/p&gt;

&lt;p&gt;This was a very welcome change and the Nikon EM served me very well. Its metering accuracy and reliability are impressive, especially knowing that it was made in 1979. However, the EM offering Aperture priority mode only, I‚Äôm sometimes limited by the absence of control of the speed and I‚Äôve been looking for a replacement but this will be the subject of another blog post.&lt;br /&gt;
In the meantime, here is a picture of a Nikon EM:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/discovery-analog-photography/Nikon_EM.jpeg&quot; alt=&quot;Nikon EM&quot; /&gt;&lt;figcaption&gt;
      Nikon EM by Neil916 at English Wikipedia, CC BY 3.0, &lt;a href=&quot;https://commons.wikimedia.org/w/index.php?curid=129511821&quot;&gt;https://commons.wikimedia.org/w/index.php?curid=129511821&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;And some of my shots with it:&lt;/p&gt;

&lt;figure class=&quot;third &quot;&gt;
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/000002.jpeg&quot; title=&quot;My first ever analog photography, Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/000002.jpeg&quot; alt=&quot;My first ever analog photography, Ilford HP5 400 ASA&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/000037.jpeg&quot; title=&quot;Karl-Marx-Platz, Berlin, f/16 1/30s Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/000037.jpeg&quot; alt=&quot;Karl-Marx-Platz, Berlin&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/DSC_0166.jpeg&quot; title=&quot;Christmas, Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/DSC_0166.jpeg&quot; alt=&quot;Christmas, Ilford HP5 400 ASA&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/DSC_0168.jpeg&quot; title=&quot;Spanish bar, Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/DSC_0168.jpeg&quot; alt=&quot;Spanish bar, Ilford HP5 400 ASA&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/DSC_0180.jpeg&quot; title=&quot;Dinner, Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/DSC_0180.jpeg&quot; alt=&quot;Dinner, Ilford HP5 400 ASA&quot; /&gt;
      &lt;/a&gt;
    
  
    
      &lt;a href=&quot;/assets/images/discovery-analog-photography/DSC_0193.jpeg&quot; title=&quot;Berlin Culture Carnival, Ilford HP5 400 ASA&quot;&gt;
          &lt;img src=&quot;/assets/images/discovery-analog-photography/DSC_0193.jpeg&quot; alt=&quot;Berlin Culture Carnival, Ilford HP5 400 ASA&quot; /&gt;
      &lt;/a&gt;
    
  
  
    &lt;figcaption&gt;For some of these shots, I scanned the negs myself, more on this later.
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Taking pictures of birds now certainly is a different kind of fun but hey, here is my first analog bird photo: a booted eagle photographed at full zoom 80mm! with auto-focus and sport mode with a Canon EOS 500 on Kodak Ultra 400ASA! I don‚Äôt know about you but I don‚Äôt see feathers or eyes‚Ä¶&lt;/p&gt;
&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/discovery-analog-photography/1197143.jpeg&quot; alt=&quot;Booted eagle&quot; /&gt;&lt;/figure&gt;</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="photography" /><category term="gear" /><summary type="html">I began using digital cameras a long time ago without trying to understand the technicalities of it until I got serious about bird photography. Then I really got into the technicalities‚Ä¶ shooting thousands of photographs trying to improve my technique and to get beautiful and tricky shots of birds. I certainly had a lot of fun doing it but at one point I needed something different, something simpler and most importantly something more creative.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/discovery-analog-photography/Nikon_EM.jpeg" /><media:content medium="image" url="http://localhost:4000/assets/images/discovery-analog-photography/Nikon_EM.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How I created this website</title><link href="http://localhost:4000/blog/website-building-1/" rel="alternate" type="text/html" title="How I created this website" /><published>2024-01-10T13:34:30+01:00</published><updated>2024-01-10T13:34:30+01:00</updated><id>http://localhost:4000/blog/website-building-1</id><content type="html" xml:base="http://localhost:4000/blog/website-building-1/">&lt;p&gt;This is my first blog article on this website, well first blog article ever I guess. And it‚Äôs about the process I followed to build it.&lt;br /&gt;
I downloaded the minimal mistake repo from github and began working in VSC. Since I built the Restore project website on the same computer a few months ago, I naively thought everything would work from the first try. I actually had a few errors but it was a great satisfaction to follow
the recommendations I left for the Restore project members, for myself and for other readers, and every thing was quickly solved and the website was running in less than an hour.&lt;/p&gt;

&lt;p&gt;Beginning summer 2023, I built the &lt;a href=&quot;https://www.globalrestoreproject.com/&quot;&gt;Restore project website&lt;/a&gt; and documented the process in a [step by step Markdown][guide_link] hosted in the &lt;a href=&quot;https://github.com/global-restore-project/grp_website&quot;&gt;Github repository.&lt;/a&gt; In the following September, I shared with my team and interested members of iDiv a more general guide on website building, the easy way using &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages.&lt;/a&gt; This newsletter is available as a blog post &lt;a href=&quot;/blog/tech-news-3/&quot;&gt;here on this website.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Changing the domain name of the Restore website from Github Pages to a dedicated domain name was another adventure‚Ä¶&lt;/p&gt;</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="webdev" /><summary type="html">This is my first blog article on this website, well first blog article ever I guess. And it‚Äôs about the process I followed to build it. I downloaded the minimal mistake repo from github and began working in VSC. Since I built the Restore project website on the same computer a few months ago, I naively thought everything would work from the first try. I actually had a few errors but it was a great satisfaction to follow the recommendations I left for the Restore project members, for myself and for other readers, and every thing was quickly solved and the website was running in less than an hour.</summary></entry><entry><title type="html">Tech news 6: Creating animated figures</title><link href="http://localhost:4000/blog/tech-news-6/" rel="alternate" type="text/html" title="Tech news 6: Creating animated figures" /><published>2023-12-19T18:28:30+01:00</published><updated>2023-12-19T18:28:30+01:00</updated><id>http://localhost:4000/blog/tech-news-6</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-6/">This week, some fun with animated figures. We send each other gifs but they rarely come to mind when it comes to showing scientific processes or results. They are actually pretty easy to make in R!

Here are a few examples using [gganimate][gganimate]. There are other packages out there but [gganimate][gganimate] is really well integrated with [ggplot2][ggplot2].

### A simple boxplot

``` r
library(ggplot2)
library(gganimate)

ggplot(mtcars, aes(factor(cyl), mpg)) + 
  geom_boxplot() + 
  # Here comes the gganimate code
  transition_states(
    gear,
    transition_length = 2,
    state_length = 1
  ) +
  enter_fade() + 
  exit_shrink() +
  ease_aes(&apos;sine-in-out‚Äô)
```

{% include figure image_path=&quot;/assets/images/tech-news-6/boxplot.gif&quot;%}
&gt; Code and figure by gganimate authors Thomas Lin Pedersen and David Robinson [^1]

### A much more informative scatter plot with evolution in time

``` r
library(gapminder)

ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +
  geom_point(alpha = 0.7, show.legend = FALSE) +
  scale_colour_manual(values = country_colors) +
  scale_size(range = c(2, 12)) +
  scale_x_log10() +
  facet_wrap(~continent) +
  # Here comes the gganimate specific bits
  labs(title = &apos;Year: {frame_time}&apos;, x = &apos;GDP per capita&apos;, y = &apos;life expectancy&apos;) +
  transition_time(year) +
  ease_aes(&apos;linear&apos;)
```

{% include figure image_path=&quot;/assets/images/tech-news-6/scatterplot.gif&quot;%}

&gt; Code and figure by gganimate authors Thomas Lin Pedersen and David Robinson [^1]

### And a map!

``` r
library(ggplot2)

center_lat &lt;- 37.8 # Some random data
center_long &lt;- -122.4
width &lt;- 0.2
num_points &lt;- 500
test_data &lt;- data.frame(&apos;lat&apos;=rnorm(num_points, mean=center_lat, sd=width),
                        &apos;long&apos;=rnorm(num_points, mean=center_long, sd=width),
                        &apos;year&apos;=floor(runif(num_points, min=1990, max=2020)),
                        &apos;temp&apos;=runif(num_points, min=-10, max=40)
)
min_long &lt;- min(test_data$long)
max_long &lt;- max(test_data$long)
min_lat &lt;- min(test_data$lat)
max_lat &lt;- max(test_data$lat)
num_years &lt;- max(test_data$year) - min(test_data$year) + 1

county_info &lt;- map_data(&quot;county&quot;, region = &quot;california&quot;) # The map background

map_with_shadow &lt;- ggplot(data = county_info,
                          mapping = aes(x = long, y = lat, group = group)) +
   geom_polygon(color = &quot;black&quot;, fill = &quot;white&quot;) +
   coord_quickmap() +
   theme_void()  +
   geom_point(data = test_data, aes(x = long, y = lat,
                                    color = temp, size = temp, group = year)) +
   coord_quickmap(xlim = c(min_long, max_long), 
                  ylim = c(min_lat, max_lat)) +
   gganimate::transition_time(year) +
   ggtitle(&apos;Year: {frame_time}&apos;,
           subtitle = &apos;Frame {frame} of {nframes}&apos;) +
   gganimate::shadow_mark()
gganimate::animate(map_with_shadow, nframes = num_years, fps = 2)
```

{% include figure image_path=&quot;/assets/images/tech-news-6/map.gif&quot;%}

&gt; Code and figure by David J. Lilja [^2]

### Great resources

- &lt;https://rquer.netlify.app/animated_chart/evolution/&gt; many examples but it‚Äôs in Spanish
- &lt;https://rquer.netlify.app/animated_map/animated_map/&gt; especially for maps

Cheers,

Alban

[^1]: Pedersen T, Robinson D (2022). gganimate: A Grammar of Animated Graphics. &lt;https://gganimate.com&gt;, &lt;https://github.com/thomasp85/gganimate&gt;.
[^2]: David J. Lilja, &quot;How to Plot and Animate Data on Maps Using the R Programming Language,&quot; University of Minnesota Digital Conservancy, &lt;https://hdl.handle.net/11299/220339&gt;, June, 2021.

[gganimate]:  https://gganimate.com/
[ggplot2]:    https://ggplot2.tidyverse.org/</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="package" /><category term="technews" /><category term="visualisation" /><summary type="html">This week, some fun with animated figures. We send each other gifs but they rarely come to mind when it comes to showing scientific processes or results. They are actually pretty easy to make in R!</summary></entry><entry><title type="html">Tech news 5: Working with data files too large for memory</title><link href="http://localhost:4000/blog/tech-news-5/" rel="alternate" type="text/html" title="Tech news 5: Working with data files too large for memory" /><published>2023-11-23T21:21:30+01:00</published><updated>2023-11-23T21:21:30+01:00</updated><id>http://localhost:4000/blog/tech-news-5</id><content type="html" xml:base="http://localhost:4000/blog/tech-news-5/">Today, I‚Äôd like to discuss data we might need that are larger than memory, or fit in memory but everything freezes‚Ä¶

It could be a very large data set such as BioTIME, GIS data, model results or simulation results that are accidentally huge but you still want to have a look to what‚Äôs inside.

Just a peek inside
First, you could have a look inside a csv by reading only parts of it: loading only some columns and/or only some rows.

``` r
R&gt; library(data.table)
R&gt; fread(file = ‚Äúdata/big_file.csv‚Äù,
         select = c(‚Äúsite‚Äù,‚Äùyear‚Äù,‚Äùtemperature‚Äù))
# select has its opposite argument drop

# Reading the column names and types ; empty columns
R&gt; fread(file = ‚Äúdata/big_file.csv‚Äù, nrows = 0)

# Reading the first 100 rows
R&gt; fread(file = ‚Äúdata/big_file.csv‚Äù, nrows = 100)

# Reading rows 10001 to 10101
R&gt; fread(file = ‚Äúdata/big_file.csv‚Äù, skip = 10000,
         nrows = 100)
```

Using factors instead of character can save quite a lot of memory space too:

``` r
R&gt; library(data.table)
R&gt; fread(file = &quot;data/communities_raw.csv&quot;,
         stringsAsFactors = FALSE) |&gt;
    object.size() / 10^6
414.4 Mbytes
R&gt; fread(file = &quot;data/communities_raw.csv&quot;,
         stringsAsFactors = TRUE) |&gt;
    object.size() / 10^6
271.1 Mbytes
```

The function is called [`fread`][fread] because it reads fast, using several cores if available, it‚Äôs very smart at guessing types and it shows a progress bar on large files.

### Smaller than memory but dplyr is slow?

Maybe your `dplyr` data wrangling step before getting analyses done is taking a few minutes or a few hours and you wouldn‚Äôt mind trying to speed things up without having to rewrite everything‚Ä¶

tidyverse developers too and they created [`dtplyr`][dtplyr] to help everyone with that. Add `library(dtplyr)` at the beginning of your script, `lazy_dt(your_data)` and bam all your `dplyr` verbs are going to be translated into `data.table` calls in the background, you won‚Äôt have to change anything else in your script‚Ä¶
`data.table` may be faster thanks to two advantages: 1) fast reimplemented functions such as `mean()` and many others and 2) the ability to make operation `by reference` ie without your column having to be copied in a different place in the memory and a new spot being booked to write the result of your operation because it‚Äôs all done in the same place in memory.

### Larger than memory using Arrow

[`Arrow`][arrow] is a cross-language multi-platform suite of tools to work on in-memory and larger-than-memory files written in C++.  
You can use it to access large files and make your usual data wrangling operations on it, even using `dplyr` verbs.

First read your data with one of the arrow functions:

- [`read_delim_arrow()`][delim]: read a delimited text file
- [`read_csv_arrow()`][delim]: read a comma-separated values (CSV) file
- [`read_tsv_arrow()`][delim]: read a tab-separated values (TSV) file
- [`read_parquet()`][parquet]: read a file in Parquet format
- [`read_feather()`][feather]: read a file in Arrow/Feather format

Arrow can read the whole data OR read some informations about the data set but without loading the data in memory, only column names, types, sizes, things like that.

Now you want to make operations on these data and R couldn‚Äôt because they don‚Äôt fit in memory but arrow is going to read your operations, translate them and execute them all at once when needed:

``` r
library(dplyr)
dset &lt;- dset %&gt;%
  group_by(subset) %&gt;%
  summarize(mean_x = mean(x), min_y = min(y)) %&gt;%
  filter(mean_x &gt; 0) %&gt;%
  arrange(subset)
# No operations were executed yet
dset %&gt;% collect() # operations are executed and results given
```

Only once `dplyr::collect()` is called the operations are ran outside of R by arrow. Meaning that the workflow can be much longer and have (much) more intermediate steps but data are loaded in memory for R only when it needs them like for plotting or running a statistical analysis.

### Spatial Data

Here we will be looking at `sf`, `stars` (for rasters) and `dbplyr` (as in databases‚Ä¶) and it is a little more advanced and specialised so I won‚Äôt go into much details but a few things I liked:
Cropping a spatial object even before loading it into R using the wkt_filter argument of `sf::st_read()`

``` r
library(sf)
file &lt;- &quot;data/nc.gpkg&quot;
c(xmin = -82,ymin = 36, xmax = -80, ymax = 37) |&gt;
    st_bbox() |&gt; st_as_sfc() |&gt; st_as_text() -&gt; bb
st_read(file, wkt_filter = bb) |&gt; nrow()
17 # out of 100
```

Even easier if you can write SQL queries directly:

``` r
q &lt;- &quot;select BIR74,SID74,geom from &apos;nc.gpkg&apos; where BIR74 &gt; 1500&quot;
read_sf(file, query = q) |&gt; nrow()
61 # out of 100
```

Using [`stars`][stars], you can read a raster file without loading it into memory, this is quite similar to `arrow` in the previous section, and a 100+Mbytes file results in a 12Mbytes object in memory in R.

### Other interesting tools

It seems that packages dedicated to running statistical models (lm, glm, etc) directly on data sets too big for memory were a thing a few years ago but I can‚Äôt find recent packages targeting this problem‚Ä¶ [`biglm`][biglm] hasn‚Äôt been updated since 2020.

### Great resources

#### dtplyr

- &lt;https://dtplyr.tidyverse.org/&gt;

#### Arrow and dplyr

- &lt;https://arrow.apache.org/docs/r/articles/arrow.html#analyzing-arrow-data-with-dplyr&gt;  
- &lt;https://arrow.apache.org/cookbook/r/index.html&gt;  
- &lt;https://arrow-user2022.netlify.app/hello-arrow&gt;  
- &lt;https://hbs-rcs.github.io/large_data_in_R/#solution-example&gt;  # examples  
- &lt;https://jthomasmock.github.io/arrow-dplyr/#/&gt;  # presentation
- &lt;https://posit-conf-2023.github.io/arrow/materials/5_arrow_single_file.html#/single-file-api&gt; # presentation  
- &lt;https://www.r-bloggers.com/2021/09/understanding-the-parquet-file-format/&gt; #parquet data format  

#### Spatial data

- &lt;https://r-spatial.org/book/09-Large.html&gt;

Happy to talk about it, share experiences, help you implement something (don‚Äôt you also think [`dtplyr`][dtplyr] sounds like a great and easy tool?!) and hear your comments!

Best wishes,

Alban

[fread]:     https://rdatatable.gitlab.io/data.table/reference/fread.html
[dtplyr]:    https://dtplyr.tidyverse.org/
[arrow]:     https://arrow.apache.org/cookbook/r/index.html
[biglm]:     https://cran.r-project.org/package=biglm
[delim]:     https://arrow.apache.org/docs/r/reference/read_delim_arrow.html
[parquet]:   https://arrow.apache.org/docs/r/reference/read_parquet.html
[feather]:   https://arrow.apache.org/docs/r/reference/read_feather.html
[stars]:     https://cran.r-project.org/package=stars</content><author><name>Alban Sagouis</name></author><category term="blog" /><category term="experience" /><category term="R" /><category term="data.table" /><category term="spatial" /><category term="dplyr" /><category term="package" /><category term="reproducibility" /><category term="technews" /><summary type="html">Today, I‚Äôd like to discuss data we might need that are larger than memory, or fit in memory but everything freezes‚Ä¶</summary></entry></feed>